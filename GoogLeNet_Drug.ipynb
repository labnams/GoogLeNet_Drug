{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pn\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend.tensorflow_backend as K\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model, Input, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, GlobalAveragePooling2D, Dense, AveragePooling2D, Flatten, Concatenate, Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset for prediction\n",
    "\n",
    "dataset = np.load('File_path//dataset.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment each dataset to variable\n",
    "\n",
    "train_x, train_y = dataset['x_train'], dataset['y_train']\n",
    "test_x, test_y = dataset['x_test'], dataset['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data to fit trained GoogLeNet model\n",
    "\n",
    "train_x_2d = train_x.reshape(-1, 167, 167, 1)\n",
    "test_x_2d = test_x.reshape(-1, 167, 167, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shape of vairable\n",
    "\n",
    "train_x_2d.shape, test_x_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment local response normalization function\n",
    "\n",
    "class LocalResponseNormalization(Layer):\n",
    "    def __init__(self, n=5, alpha=1e-4, beta=0.75, k=2, **kwargs):\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.k = k\n",
    "        super(LocalResponseNormalization, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.shape = input_shape\n",
    "        super(LocalResponseNormalization, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        _, r, c, f = self.shape\n",
    "        squared = K.square(x)\n",
    "        pooled = K.pool2d(squared, (self.n, self.n), strides=(1,1), padding=\"same\", pool_mode='avg')\n",
    "        summed = K.sum(pooled, axis=3, keepdims=True)\n",
    "        averaged = self.alpha * K.repeat_elements(summed, f, axis=3)\n",
    "        denom = K.pow(self.k + averaged, self.beta)\n",
    "        return x / denom\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For save model\n",
    "\n",
    "callback_list_main = [\n",
    "        keras.callbacks.ModelCheckpoint( # save weight per epoch\n",
    "        filepath=\"File_path//GoogLeNet.h5\", # model save directory\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment inception module\n",
    "\n",
    "def inception(input_tensor, filter_channels):\n",
    "    filter_1x1, filter_3x3_R, filter_3x3, filter_5x5_R, filter_5x5, pool_proj = filter_channels\n",
    "\n",
    "    branch_1 = Conv2D(filter_1x1, (1, 1), strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(input_tensor)\n",
    "    branch_2 = Conv2D(filter_3x3_R, (1, 1), strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(input_tensor)\n",
    "    branch_2 = Conv2D(filter_3x3, (3, 3), strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(branch_2)\n",
    "    branch_3 = Conv2D(filter_5x5_R, (1, 1), strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(input_tensor)\n",
    "    branch_3 = Conv2D(filter_5x5, (5, 5), strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(branch_3)\n",
    "    branch_4 = MaxPooling2D((3, 3), strides=1, padding='same')(input_tensor)\n",
    "    branch_4 = Conv2D(pool_proj, (1, 1), strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(branch_4)\n",
    "    DepthConcat = Concatenate()([branch_1, branch_2, branch_3, branch_4])\n",
    "    \n",
    "    return DepthConcat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of GoogLeNet\n",
    "\n",
    "def GoogLeNet(model_input, classes=3):\n",
    "    conv_1 = Conv2D(64, (7, 7), strides=2, padding='same', activation='relu')(model_input)\n",
    "    pool_1 = MaxPooling2D((3, 3), strides=2, padding='same')(conv_1)\n",
    "    LRN_1 = LocalResponseNormalization()(pool_1)\n",
    "    \n",
    "    conv_2 = Conv2D(64, (1, 1), strides=1, padding='valid', activation='relu')(LRN_1)\n",
    "    conv_3 = Conv2D(192, (3, 3), strides=1, padding='same', activation='relu')(conv_2)\n",
    "    LRN_2 = LocalResponseNormalization()(conv_3)\n",
    "    pool_2 = MaxPooling2D((3, 3), strides=2, padding='same')(LRN_2)\n",
    "    \n",
    "    inception_3a = inception(pool_2, [64, 96, 128, 16, 32, 32])\n",
    "    inception_3b = inception(inception_3a, [128, 128, 192, 32, 96, 64])\n",
    "    pool_3 = MaxPooling2D((3, 3), strides=2, padding='same')(inception_3b)\n",
    "    \n",
    "    inception_4a = inception(pool_3, [192, 96, 208, 16, 48, 64])\n",
    "    inception_4b = inception(inception_4a, [160, 112, 224, 24, 64, 64])\n",
    "    inception_4c = inception(inception_4b, [128, 128, 256, 24, 64, 64])\n",
    "    inception_4d = inception(inception_4c, [112, 144, 288, 32, 64, 64])\n",
    "    inception_4e = inception(inception_4d, [256, 160, 320, 32, 128, 128])\n",
    "    pool_4 = MaxPooling2D((3, 3), strides=2, padding='same')(inception_4e)\n",
    "    \n",
    "    inception_5a = inception(pool_4, [256, 160, 320, 32, 128, 128])\n",
    "    inception_5b = inception(inception_5a, [384, 192, 384, 48, 128, 128])\n",
    "    avg_pool = GlobalAveragePooling2D()(inception_5b)\n",
    "    dropout = Dropout(0.4)(avg_pool)\n",
    "    \n",
    "    linear = Dense(1000, activation='relu')(dropout)\n",
    "    model_output = Dense(classes, activation='softmax', name='main_classifier')(linear) # 'softmax'\n",
    "    \n",
    "    # Auxiliary Classifier\n",
    "\n",
    "    auxiliary_4a = AveragePooling2D((5, 5), strides=3, padding='valid')(inception_4a)\n",
    "    auxiliary_4a = Conv2D(128, (1, 1), strides=1, padding='same', activation='relu')(auxiliary_4a)\n",
    "    auxiliary_4a = Flatten()(auxiliary_4a)\n",
    "    auxiliary_4a = Dense(1024, activation='relu')(auxiliary_4a)\n",
    "    auxiliary_4a = Dropout(0.7)(auxiliary_4a)\n",
    "    auxiliary_4a = Dense(classes, activation='softmax', name='auxiliary_4a')(auxiliary_4a)\n",
    "    \n",
    "    auxiliary_4d = AveragePooling2D((5, 5), strides=3, padding='valid')(inception_4d)\n",
    "    auxiliary_4d = Conv2D(128, (1, 1), strides=1, padding='same', activation='relu')(auxiliary_4d)\n",
    "    auxiliary_4d = Flatten()(auxiliary_4d)\n",
    "    auxiliary_4d = Dense(1024, activation='relu')(auxiliary_4d)\n",
    "    auxiliary_4d = Dropout(0.7)(auxiliary_4d)\n",
    "    auxiliary_4d = Dense(classes, activation='softmax', name='auxiliary_4d')(auxiliary_4d)\n",
    "    \n",
    "    model = Model(model_input, [model_output, auxiliary_4a, auxiliary_4d])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile GoogLeNet model\n",
    "\n",
    "input_shape = (167, 167, 1)    \n",
    "model_input = Input( shape=input_shape )\n",
    "model = GoogLeNet(model_input, 3)\n",
    "optimizer = SGD(momentum=0.9)\n",
    "\n",
    "model.compile(optimizer, \n",
    "              loss={'main_classifier' : 'categorical_crossentropy',\n",
    "                    'auxiliary_4a' : 'categorical_crossentropy',\n",
    "                    'auxiliary_4d' : 'categorical_crossentropy'},\n",
    "              loss_weights={'main_classifier' : 1.0,\n",
    "                            'auxiliary_4a' : 0.3,\n",
    "                            'auxiliary_4d' : 0.3}, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of GoogLeNet model\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training step\n",
    "\n",
    "StartTime4 = datetime.now()\n",
    "print(\"StartTime :\", StartTime4)\n",
    "\n",
    "with K.tf.device('/GPU:0'):\n",
    "    history = model.fit(train_x_2d, {'main_classifier' : train_y, 'auxiliary_4a' : train_y, 'auxiliary_4d' : train_y},\n",
    "              epochs=250, batch_size=300, callbacks = callback_list_main)\n",
    "\n",
    "              \n",
    "EndTime4 = datetime.now()\n",
    "print(\"EndTime :\", EndTime4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model accuracy\n",
    "\n",
    "test_eval = model.evaluate(test_x_2d, [test_y, test_y, test_y], verbose=1)\n",
    "\n",
    "print('\\nGoogLeNet GDSC Accuracy: {:.4f}'.format(test_eval[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction step : Result is probability value for each class\n",
    "\n",
    "predict_prob_y = model.predict(test_x_2d, batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert probability value to prediction value.\n",
    "\n",
    "predict_y = np.argmax(predict_prob_y[0], axis=1) \n",
    "conf_y = np.argmax(test_y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confusion matrix of prediction result\n",
    "\n",
    "confusion_matrix(conf_y, prediction_y, labels=[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose main prediction value\n",
    "\n",
    "main_y = predict_prob_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Number of Clases \n",
    "n_classes = 3\n",
    "\n",
    "# Compute ROC curve and ROC area for three classes\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(test_y[:, i], main_y[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr["micro"], tpr["micro"], _ = roc_curve(test_y.ravel(), main_y.ravel())\n",
    "roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])\n"    
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each class' ROC curve\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(dpi=400)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='Micro-average (AUROC = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='#f7b538'\n",
    "         , linewidth=2\n",
    "        )\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='Macro-average (AUROC = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='#780116'\n",
    "         , linewidth=2\n",
    "        )\n",
    "\n",
    "colors = cycle(['#db7c26', '#d8572a', '#c32f27'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=1.5,\n",
    "             label='Class {0} (AUROC = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='grey',linestyle= '--', lw=1)\n",
    "plt.xticks([0, 0.5, 1.0])\n",
    "plt.yticks([0, 0.5, 1.0])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve in the GDSC test set')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
